{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# happyleaf\n",
    "# main.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import model as m\n",
    "\n",
    "np.random.seed(41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes: train 49188, val 6149, test 6149\n",
      "Categories: 39\n"
     ]
    }
   ],
   "source": [
    "with open(\"numpydata.pkl\", \"rb\") as f:\n",
    "    (xs_all, data_indices_all) = pickle.load(f)\n",
    "\n",
    "img_size = xs_all.shape[1]\n",
    "total_img_num = xs_all.shape[0]\n",
    "num_categories = data_indices_all.shape[1]\n",
    "\n",
    "def unison_shuffled(a, b):\n",
    "    rng_state = np.random.get_state()\n",
    "    np.random.shuffle(a)\n",
    "    np.random.set_state(rng_state)\n",
    "    np.random.shuffle(b)\n",
    "\n",
    "def prepare_batch(xs_h, data_indices_h, u, v):\n",
    "    len_tot = xs_h.shape[0]\n",
    "    u_tr, v_tr = u % len_tot, v % len_tot\n",
    "    if v_tr + len_tot - u_tr == v - u:\n",
    "        xs_first = xs_h[u_tr:len_tot, ...]\n",
    "        d_first = data_indices_h[u_tr:len_tot, ...]\n",
    "        unison_shuffled(xs_h, data_indices_h)\n",
    "        xs_second = xs_h[0:v_tr, ...]\n",
    "        d_second = data_indices_h[0:v_tr, ...]\n",
    "        xs_o = np.concatenate([xs_first, xs_second])\n",
    "        data_indices_o = np.concatenate([d_first, d_second])\n",
    "    else:\n",
    "       xs_o = xs_h[u_tr:v_tr, ...]\n",
    "       data_indices_o = data_indices_h[u_tr:v_tr, ...]\n",
    "    return jnp.array(xs_o), jnp.array(data_indices_o)\n",
    "\n",
    "unison_shuffled(xs_all, data_indices_all)\n",
    "\n",
    "u, v = int(0.8 * total_img_num), int(0.9 * total_img_num)\n",
    "xs_train, data_indices_train = xs_all[:u, ...], data_indices_all[:u, ...]\n",
    "xs_val, data_indices_val = xs_all[u:v, ...], data_indices_all[u:v, ...]\n",
    "xs_test, data_indices_test = xs_all[v:, ...], data_indices_all[v:, ...]\n",
    "\n",
    "print(f\"Dataset sizes: train {u}, val {v - u}, test {total_img_num - v}\")\n",
    "print(f\"Categories: {num_categories}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 119845\n",
      "Random picks: loss 3.6635615825653076, classify = 0.02564102564102564\n"
     ]
    }
   ],
   "source": [
    "model = m.MainModel()\n",
    "\n",
    "model.val_arr = [[jnp.log(num_categories), 1 / num_categories]]\n",
    "model.train_arr = model.val_arr.copy()\n",
    "model.test_loss = 0.0\n",
    "model.test_classify = 0.0\n",
    "\n",
    "total = 50000\n",
    "batch_size = 64\n",
    "\n",
    "print(f\"Number of parameters: {model.total_params()}\")\n",
    "print(f\"Random picks: loss {model.val_arr[0][0]}, classify = {model.val_arr[0][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 6400: train loss 3.2320704460144043, classify 0.16348710656166077\n",
      "Run 6400: val loss 3.184725522994995, classify 0.18490125238895416\n",
      "Run 12800: train loss 2.5554802417755127, classify 0.30821493268013\n",
      "Run 12800: val loss 2.604909896850586, classify 0.30091315507888794\n",
      "Run 19200: train loss 2.221062421798706, classify 0.38603752851486206\n",
      "Run 19200: val loss 2.2465567588806152, classify 0.3901450037956238\n",
      "Run 25600: train loss 2.0874619483947754, classify 0.412085622549057\n",
      "Run 25600: val loss 2.194631814956665, classify 0.3840399384498596\n",
      "Run 32000: train loss 2.044651985168457, classify 0.43219929933547974\n",
      "Run 32000: val loss 2.049119234085083, classify 0.42427465319633484\n",
      "Run 38400: train loss 1.862437129020691, classify 0.46506622433662415\n",
      "Run 38400: val loss 2.044295310974121, classify 0.4007043242454529\n",
      "Run 44800: train loss 1.898861050605774, classify 0.45580002665519714\n",
      "Run 44800: val loss 1.903336524963379, classify 0.471967488527298\n",
      "Run 51200: train loss 1.7217243909835815, classify 0.5036375522613525\n",
      "Run 51200: val loss 1.7692985534667969, classify 0.4893099069595337\n",
      "Run 57600: train loss 1.7051430940628052, classify 0.4992077350616455\n",
      "Run 57600: val loss 1.7126920223236084, classify 0.5165145993232727\n",
      "Run 64000: train loss 1.5676965713500977, classify 0.5460517406463623\n",
      "Run 64000: val loss 1.634942650794983, classify 0.5348988771438599\n",
      "Run 70400: train loss 1.5009742975234985, classify 0.5750079154968262\n",
      "Run 70400: val loss 1.6662132740020752, classify 0.5151540040969849\n",
      "Run 76800: train loss 1.7397181987762451, classify 0.5133612751960754\n",
      "Run 76800: val loss 1.7804347276687622, classify 0.4994417428970337\n",
      "Run 83200: train loss 1.5450938940048218, classify 0.5519695281982422\n",
      "Run 83200: val loss 1.5948314666748047, classify 0.5458436012268066\n",
      "Run 89600: train loss 1.276500940322876, classify 0.6289435029029846\n",
      "Run 89600: val loss 1.3779042959213257, classify 0.6045566201210022\n",
      "Run 96000: train loss 1.3560760021209717, classify 0.60830157995224\n",
      "Run 96000: val loss 1.396073579788208, classify 0.5783818960189819\n",
      "Run 102400: train loss 1.360411286354065, classify 0.5942025184631348\n",
      "Run 102400: val loss 1.3367936611175537, classify 0.6242411732673645\n",
      "Run 108800: train loss 1.2109752893447876, classify 0.6434664130210876\n",
      "Run 108800: val loss 1.2175663709640503, classify 0.647705078125\n",
      "Run 115200: train loss 1.1374913454055786, classify 0.6455919742584229\n",
      "Run 115200: val loss 1.1996585130691528, classify 0.6415049433708191\n",
      "Run 121600: train loss 1.0834554433822632, classify 0.6634782552719116\n",
      "Run 121600: val loss 1.2342493534088135, classify 0.6458477973937988\n",
      "Run 128000: train loss 1.1595089435577393, classify 0.6469396352767944\n",
      "Run 128000: val loss 1.1433056592941284, classify 0.6562364101409912\n",
      "Run 134400: train loss 1.1259785890579224, classify 0.6585007905960083\n",
      "Run 134400: val loss 1.1523739099502563, classify 0.6607500910758972\n",
      "Run 140800: train loss 0.9813543558120728, classify 0.721312940120697\n",
      "Run 140800: val loss 1.0479048490524292, classify 0.6863048672676086\n",
      "Run 147200: train loss 0.9288166761398315, classify 0.7156047224998474\n",
      "Run 147200: val loss 0.9854759573936462, classify 0.710801899433136\n",
      "Run 153600: train loss 0.8760679364204407, classify 0.740986704826355\n",
      "Run 153600: val loss 0.9955412745475769, classify 0.7031466960906982\n",
      "Run 160000: train loss 0.9154549241065979, classify 0.7364157438278198\n",
      "Run 160000: val loss 0.9839155077934265, classify 0.714671790599823\n",
      "Run 166400: train loss 0.9069797396659851, classify 0.7344728708267212\n",
      "Run 166400: val loss 0.9626189470291138, classify 0.7071197032928467\n",
      "Run 172800: train loss 0.9789785146713257, classify 0.7105599045753479\n",
      "Run 172800: val loss 1.0148530006408691, classify 0.7019807696342468\n",
      "Run 179200: train loss 0.9431983232498169, classify 0.7149109244346619\n",
      "Run 179200: val loss 0.9739474654197693, classify 0.6943711042404175\n",
      "Run 185600: train loss 0.8987048268318176, classify 0.7116003632545471\n",
      "Run 185600: val loss 0.9602594375610352, classify 0.7042519450187683\n",
      "Run 192000: train loss 0.9452216625213623, classify 0.7269673943519592\n",
      "Run 192000: val loss 0.9618878960609436, classify 0.7063603401184082\n",
      "Run 198400: train loss 0.8753254413604736, classify 0.7330626249313354\n",
      "Run 198400: val loss 1.0133589506149292, classify 0.702663004398346\n",
      "Run 204800: train loss 0.8882871270179749, classify 0.7441530227661133\n",
      "Run 204800: val loss 0.9299227595329285, classify 0.7118581533432007\n",
      "Run 211200: train loss 0.788331925868988, classify 0.7561062574386597\n",
      "Run 211200: val loss 0.8985465168952942, classify 0.7351539134979248\n",
      "Run 217600: train loss 0.8910530805587769, classify 0.7447472214698792\n",
      "Run 217600: val loss 0.9576046466827393, classify 0.717229425907135\n",
      "Run 224000: train loss 0.8671792149543762, classify 0.7398039698600769\n",
      "Run 224000: val loss 0.8760822415351868, classify 0.7390165328979492\n",
      "Run 230400: train loss 0.8247417211532593, classify 0.7427989840507507\n",
      "Run 230400: val loss 0.9181345701217651, classify 0.7404462099075317\n",
      "Run 236800: train loss 0.8668981790542603, classify 0.7167565822601318\n",
      "Run 236800: val loss 0.9364238977432251, classify 0.7217383980751038\n",
      "Run 243200: train loss 0.9496256113052368, classify 0.7234798669815063\n",
      "Run 243200: val loss 0.933878481388092, classify 0.7183553576469421\n",
      "Run 249600: train loss 0.8684629797935486, classify 0.7337520122528076\n",
      "Run 249600: val loss 0.8798006176948547, classify 0.7323124408721924\n",
      "Run 256000: train loss 0.8325909376144409, classify 0.7649840116500854\n",
      "Run 256000: val loss 0.9848909974098206, classify 0.7132614850997925\n",
      "Run 262400: train loss 0.8600623607635498, classify 0.7421647906303406\n",
      "Run 262400: val loss 0.9223167300224304, classify 0.7333576679229736\n",
      "Run 268800: train loss 0.8245449662208557, classify 0.7511623501777649\n",
      "Run 268800: val loss 0.9252456426620483, classify 0.7271217107772827\n",
      "Run 275200: train loss 0.7959882020950317, classify 0.7557408213615417\n",
      "Run 275200: val loss 0.8613690733909607, classify 0.7442812323570251\n",
      "Run 281600: train loss 0.8385927677154541, classify 0.7588002681732178\n",
      "Run 281600: val loss 0.849132239818573, classify 0.7455536127090454\n",
      "Run 288000: train loss 0.8233535885810852, classify 0.7471749782562256\n",
      "Run 288000: val loss 0.9051699638366699, classify 0.7328435778617859\n",
      "Run 294400: train loss 0.8357648849487305, classify 0.7303770184516907\n",
      "Run 294400: val loss 0.9279090166091919, classify 0.732406735420227\n",
      "Run 300800: train loss 0.7969493269920349, classify 0.7379471063613892\n",
      "Run 300800: val loss 0.8195750713348389, classify 0.7465742826461792\n",
      "Run 307200: train loss 0.7907588481903076, classify 0.7707352042198181\n",
      "Run 307200: val loss 0.9384236335754395, classify 0.7218360900878906\n",
      "Run 313600: train loss 0.8190810084342957, classify 0.7559675574302673\n",
      "Run 313600: val loss 0.9146304726600647, classify 0.7251713871955872\n",
      "Run 320000: train loss 0.8034366369247437, classify 0.7471883296966553\n",
      "Run 320000: val loss 0.9225522875785828, classify 0.7336041331291199\n",
      "Run 326400: train loss 0.7979485988616943, classify 0.7524677515029907\n",
      "Run 326400: val loss 0.8396461606025696, classify 0.7406577467918396\n",
      "Run 332800: train loss 0.7189529538154602, classify 0.7762479186058044\n",
      "Run 332800: val loss 0.867268979549408, classify 0.7379751801490784\n",
      "Run 339200: train loss 0.8195303082466125, classify 0.7568193078041077\n",
      "Run 339200: val loss 0.8113737106323242, classify 0.7508975267410278\n",
      "Run 345600: train loss 0.6848711371421814, classify 0.787361741065979\n",
      "Run 345600: val loss 0.8375234007835388, classify 0.7533324956893921\n",
      "Run 352000: train loss 0.7129809260368347, classify 0.788126528263092\n",
      "Run 352000: val loss 0.8605663776397705, classify 0.7358996272087097\n",
      "Run 358400: train loss 0.7444970607757568, classify 0.770434558391571\n",
      "Run 358400: val loss 0.8437084555625916, classify 0.7433515191078186\n",
      "Run 364800: train loss 0.7418229579925537, classify 0.7814932465553284\n",
      "Run 364800: val loss 0.7567553520202637, classify 0.7646357417106628\n",
      "Run 371200: train loss 0.7300935387611389, classify 0.7662277221679688\n",
      "Run 371200: val loss 0.858762264251709, classify 0.7333488464355469\n",
      "Run 377600: train loss 0.7384088039398193, classify 0.7672811150550842\n",
      "Run 377600: val loss 0.8277168273925781, classify 0.7521644234657288\n",
      "Run 384000: train loss 0.777302086353302, classify 0.7638338804244995\n",
      "Run 384000: val loss 0.8606698513031006, classify 0.7372784614562988\n",
      "Run 390400: train loss 0.7575473785400391, classify 0.7722625136375427\n",
      "Run 390400: val loss 0.8214274644851685, classify 0.7578554749488831\n",
      "Run 396800: train loss 0.7618696689605713, classify 0.7804858088493347\n",
      "Run 396800: val loss 0.8114691972732544, classify 0.7541115880012512\n",
      "Run 403200: train loss 0.7536798715591431, classify 0.7647570371627808\n",
      "Run 403200: val loss 0.8178651928901672, classify 0.7491315603256226\n",
      "Run 409600: train loss 0.6842304468154907, classify 0.8017911314964294\n",
      "Run 409600: val loss 0.8319538831710815, classify 0.750973105430603\n",
      "Run 416000: train loss 0.7685927748680115, classify 0.7624668478965759\n",
      "Run 416000: val loss 0.8027498126029968, classify 0.7506610155105591\n",
      "Run 422400: train loss 0.7243132591247559, classify 0.7927782535552979\n",
      "Run 422400: val loss 0.8566493988037109, classify 0.7577238082885742\n",
      "Run 428800: train loss 0.7881086468696594, classify 0.7500156760215759\n",
      "Run 428800: val loss 0.7852405905723572, classify 0.7678723335266113\n",
      "Run 435200: train loss 0.6815923452377319, classify 0.7947113513946533\n",
      "Run 435200: val loss 0.8146992921829224, classify 0.76746666431427\n",
      "Run 441600: train loss 0.7278323769569397, classify 0.7787959575653076\n",
      "Run 441600: val loss 0.8411492109298706, classify 0.7430896759033203\n",
      "Run 448000: train loss 0.7139349579811096, classify 0.7753345966339111\n",
      "Run 448000: val loss 0.8030757308006287, classify 0.7560326457023621\n",
      "Run 454400: train loss 0.7098915576934814, classify 0.784700334072113\n",
      "Run 454400: val loss 0.788206934928894, classify 0.7590321898460388\n",
      "Run 460800: train loss 0.7342271208763123, classify 0.7777852416038513\n",
      "Run 460800: val loss 0.8534482717514038, classify 0.7536031007766724\n",
      "Run 467200: train loss 0.723980724811554, classify 0.7769832015037537\n",
      "Run 467200: val loss 0.8374927043914795, classify 0.7466140985488892\n",
      "Run 473600: train loss 0.6648184061050415, classify 0.7928960919380188\n",
      "Run 473600: val loss 0.762306809425354, classify 0.7678717374801636\n",
      "Run 480000: train loss 0.7309920787811279, classify 0.7875687479972839\n",
      "Run 480000: val loss 0.8533787727355957, classify 0.7490334510803223\n",
      "Run 486400: train loss 0.7070120573043823, classify 0.7870202660560608\n",
      "Run 486400: val loss 0.7775771021842957, classify 0.7662864327430725\n",
      "Run 492800: train loss 0.697451114654541, classify 0.8093450665473938\n",
      "Run 492800: val loss 0.7479895353317261, classify 0.7819276452064514\n",
      "Run 499200: train loss 0.7206761837005615, classify 0.7860567569732666\n",
      "Run 499200: val loss 0.811216413974762, classify 0.764732837677002\n",
      "Run 505600: train loss 0.662459671497345, classify 0.800195574760437\n",
      "Run 505600: val loss 0.8163941502571106, classify 0.7631930708885193\n",
      "Run 512000: train loss 0.6801864504814148, classify 0.791718602180481\n",
      "Run 512000: val loss 0.8171473145484924, classify 0.7560689449310303\n",
      "Run 518400: train loss 0.7174041271209717, classify 0.7727904319763184\n",
      "Run 518400: val loss 0.7855310440063477, classify 0.7667993903160095\n",
      "Run 524800: train loss 0.6805651783943176, classify 0.7953658103942871\n",
      "Run 524800: val loss 0.8645713329315186, classify 0.7528142929077148\n",
      "Run 531200: train loss 0.67982417345047, classify 0.7990167737007141\n",
      "Run 531200: val loss 0.8324536085128784, classify 0.7614786624908447\n",
      "Run 537600: train loss 0.6507624983787537, classify 0.8003115057945251\n",
      "Run 537600: val loss 0.7765911221504211, classify 0.7523273229598999\n",
      "Run 544000: train loss 0.6934290528297424, classify 0.7828763723373413\n",
      "Run 544000: val loss 0.8014214038848877, classify 0.7479692697525024\n",
      "Run 550400: train loss 0.7481005191802979, classify 0.7641218304634094\n",
      "Run 550400: val loss 0.7755040526390076, classify 0.779080867767334\n",
      "Run 556800: train loss 0.6481159329414368, classify 0.8066221475601196\n",
      "Run 556800: val loss 0.7509269714355469, classify 0.7672760486602783\n",
      "Run 563200: train loss 0.7080376148223877, classify 0.794467031955719\n",
      "Run 563200: val loss 0.8476172685623169, classify 0.748482882976532\n",
      "Run 569600: train loss 0.6266357898712158, classify 0.8267320990562439\n",
      "Run 569600: val loss 0.7905858755111694, classify 0.7735864520072937\n",
      "Run 576000: train loss 0.6799628734588623, classify 0.7944397330284119\n",
      "Run 576000: val loss 0.7823390960693359, classify 0.7599749565124512\n",
      "Run 582400: train loss 0.6809259057044983, classify 0.7868450284004211\n",
      "Run 582400: val loss 0.8309121131896973, classify 0.7536135315895081\n",
      "Run 588800: train loss 0.6394560933113098, classify 0.8056809306144714\n",
      "Run 588800: val loss 0.7695326805114746, classify 0.7546599507331848\n",
      "Run 595200: train loss 0.6713347434997559, classify 0.7947655916213989\n",
      "Run 595200: val loss 0.8087660670280457, classify 0.7625266313552856\n",
      "Run 601600: train loss 0.6227337718009949, classify 0.8219388723373413\n",
      "Run 601600: val loss 0.7570403814315796, classify 0.7696186304092407\n",
      "Run 608000: train loss 0.7096468210220337, classify 0.7958271503448486\n",
      "Run 608000: val loss 0.8322957158088684, classify 0.7431284189224243\n",
      "Run 614400: train loss 0.6630049347877502, classify 0.7986961603164673\n",
      "Run 614400: val loss 0.769625723361969, classify 0.7699722647666931\n",
      "Run 620800: train loss 0.6912665963172913, classify 0.803380012512207\n",
      "Run 620800: val loss 0.7979424595832825, classify 0.764595091342926\n",
      "Run 627200: train loss 0.7040038704872131, classify 0.7839323282241821\n",
      "Run 627200: val loss 0.7302647233009338, classify 0.7850923538208008\n",
      "Run 633600: train loss 0.6655247807502747, classify 0.8013512492179871\n",
      "Run 633600: val loss 0.7615177631378174, classify 0.7685902118682861\n",
      "Run 640000: train loss 0.6626355051994324, classify 0.7966577410697937\n",
      "Run 640000: val loss 0.7526939511299133, classify 0.7763044238090515\n",
      "Run 646400: train loss 0.6348816752433777, classify 0.8179672360420227\n",
      "Run 646400: val loss 0.8009944558143616, classify 0.7615944743156433\n",
      "Run 652800: train loss 0.693393886089325, classify 0.803628146648407\n",
      "Run 652800: val loss 0.8131343126296997, classify 0.7444603443145752\n",
      "Run 659200: train loss 0.6300193071365356, classify 0.8123467564582825\n",
      "Run 659200: val loss 0.728760838508606, classify 0.7802690267562866\n",
      "Run 665600: train loss 0.6299782991409302, classify 0.818921685218811\n",
      "Run 665600: val loss 0.7377573251724243, classify 0.7746027708053589\n",
      "Run 672000: train loss 0.6173598170280457, classify 0.8116440773010254\n",
      "Run 672000: val loss 0.6948198080062866, classify 0.7998766899108887\n",
      "Run 678400: train loss 0.6031116843223572, classify 0.8148326873779297\n",
      "Run 678400: val loss 0.791693925857544, classify 0.7730246186256409\n",
      "Run 684800: train loss 0.68776535987854, classify 0.785319447517395\n",
      "Run 684800: val loss 0.7390865683555603, classify 0.7721133828163147\n",
      "Run 691200: train loss 0.5947192311286926, classify 0.8242626190185547\n",
      "Run 691200: val loss 0.6493790149688721, classify 0.8022699356079102\n",
      "Run 697600: train loss 0.6705122590065002, classify 0.7908174991607666\n",
      "Run 697600: val loss 0.7678864598274231, classify 0.75688636302948\n",
      "Run 704000: train loss 0.6734150648117065, classify 0.8079270720481873\n",
      "Run 704000: val loss 0.7673999071121216, classify 0.7580218315124512\n",
      "Run 710400: train loss 0.6772050261497498, classify 0.8031650185585022\n",
      "Run 710400: val loss 0.7698944807052612, classify 0.7645218372344971\n",
      "Run 716800: train loss 0.7053467035293579, classify 0.7809308171272278\n",
      "Run 716800: val loss 0.7448358535766602, classify 0.7715396285057068\n",
      "Run 723200: train loss 0.6392618417739868, classify 0.8066489100456238\n",
      "Run 723200: val loss 0.7900535464286804, classify 0.7653323411941528\n",
      "Run 729600: train loss 0.658149003982544, classify 0.8070653676986694\n",
      "Run 729600: val loss 0.7616333365440369, classify 0.7712574005126953\n",
      "Run 736000: train loss 0.5833449959754944, classify 0.8287695646286011\n",
      "Run 736000: val loss 0.7329105138778687, classify 0.7582894563674927\n",
      "Run 742400: train loss 0.6348872780799866, classify 0.8099051713943481\n",
      "Run 742400: val loss 0.703861653804779, classify 0.782500684261322\n",
      "Run 748800: train loss 0.6193408966064453, classify 0.7972915768623352\n",
      "Run 748800: val loss 0.7228043079376221, classify 0.7761269807815552\n",
      "Run 755200: train loss 0.6459839940071106, classify 0.7972766757011414\n",
      "Run 755200: val loss 0.7725764513015747, classify 0.7621281147003174\n",
      "Run 761600: train loss 0.6032413244247437, classify 0.8184261322021484\n",
      "Run 761600: val loss 0.6690518856048584, classify 0.7990776896476746\n",
      "Run 768000: train loss 0.6075275540351868, classify 0.8104434609413147\n",
      "Run 768000: val loss 0.7377385497093201, classify 0.7747281193733215\n",
      "Run 774400: train loss 0.5995111465454102, classify 0.8152354955673218\n",
      "Run 774400: val loss 0.7488390803337097, classify 0.7698464393615723\n",
      "Run 780800: train loss 0.6070476770401001, classify 0.8244351148605347\n",
      "Run 780800: val loss 0.7215671539306641, classify 0.7805233001708984\n",
      "Run 787200: train loss 0.6348695755004883, classify 0.8133781552314758\n",
      "Run 787200: val loss 0.7373218536376953, classify 0.7793849110603333\n",
      "Run 793600: train loss 0.6163424253463745, classify 0.8135323524475098\n",
      "Run 793600: val loss 0.6408013701438904, classify 0.8036554455757141\n",
      "Run 800000: train loss 0.5868350863456726, classify 0.8188167810440063\n",
      "Run 800000: val loss 0.7521953582763672, classify 0.758129894733429\n",
      "Run 806400: train loss 0.5839554667472839, classify 0.8226719498634338\n",
      "Run 806400: val loss 0.7359867691993713, classify 0.7776986360549927\n",
      "Run 812800: train loss 0.5908167362213135, classify 0.8047615885734558\n",
      "Run 812800: val loss 0.7072829604148865, classify 0.7850306630134583\n",
      "Run 819200: train loss 0.6309947371482849, classify 0.8178153038024902\n",
      "Run 819200: val loss 0.6941027641296387, classify 0.796441912651062\n",
      "Run 825600: train loss 0.6460126042366028, classify 0.8025703430175781\n",
      "Run 825600: val loss 0.7784913778305054, classify 0.769156813621521\n",
      "Run 832000: train loss 0.6582980751991272, classify 0.7948048114776611\n",
      "Run 832000: val loss 0.7340261340141296, classify 0.7802304625511169\n",
      "Run 838400: train loss 0.5966571569442749, classify 0.8167185187339783\n",
      "Run 838400: val loss 0.67936110496521, classify 0.7911633849143982\n",
      "Run 844800: train loss 0.5854359865188599, classify 0.8274034261703491\n",
      "Run 844800: val loss 0.7747430801391602, classify 0.7627223134040833\n",
      "Run 851200: train loss 0.5967645049095154, classify 0.8165854215621948\n",
      "Run 851200: val loss 0.672433078289032, classify 0.7917208671569824\n",
      "Run 857600: train loss 0.5675076842308044, classify 0.8325868844985962\n",
      "Run 857600: val loss 0.6804835200309753, classify 0.7936201691627502\n",
      "Run 864000: train loss 0.6228153705596924, classify 0.8167852759361267\n",
      "Run 864000: val loss 0.7360915541648865, classify 0.7720308303833008\n",
      "Run 870400: train loss 0.5873905420303345, classify 0.8193042278289795\n",
      "Run 870400: val loss 0.7344867587089539, classify 0.7683671712875366\n",
      "Run 876800: train loss 0.5919833779335022, classify 0.8187989592552185\n",
      "Run 876800: val loss 0.6969236135482788, classify 0.7742936015129089\n",
      "Run 883200: train loss 0.5877554416656494, classify 0.8214715123176575\n",
      "Run 883200: val loss 0.7460954785346985, classify 0.7724490165710449\n",
      "Run 889600: train loss 0.6540970206260681, classify 0.782502293586731\n",
      "Run 889600: val loss 0.6764834523200989, classify 0.7986639142036438\n",
      "Run 896000: train loss 0.6290651559829712, classify 0.8163473606109619\n",
      "Run 896000: val loss 0.6486296057701111, classify 0.7982569932937622\n",
      "Run 902400: train loss 0.5548934936523438, classify 0.8172866106033325\n",
      "Run 902400: val loss 0.701123833656311, classify 0.7776904702186584\n",
      "Run 908800: train loss 0.6175625324249268, classify 0.8134589791297913\n",
      "Run 908800: val loss 0.6545652747154236, classify 0.7991381883621216\n",
      "Run 915200: train loss 0.5648491978645325, classify 0.8253316283226013\n",
      "Run 915200: val loss 0.6924712061882019, classify 0.784357488155365\n",
      "Run 921600: train loss 0.6051534414291382, classify 0.8137809038162231\n",
      "Run 921600: val loss 0.7864121794700623, classify 0.7606671452522278\n",
      "Run 928000: train loss 0.6028560400009155, classify 0.8141663670539856\n",
      "Run 928000: val loss 0.6789618730545044, classify 0.7964940667152405\n",
      "Run 934400: train loss 0.5189098715782166, classify 0.8409861922264099\n",
      "Run 934400: val loss 0.7110255360603333, classify 0.7808087468147278\n",
      "Run 940800: train loss 0.5953042507171631, classify 0.829890251159668\n",
      "Run 940800: val loss 0.6629705429077148, classify 0.7993287444114685\n",
      "Run 947200: train loss 0.5722452998161316, classify 0.8193630576133728\n",
      "Run 947200: val loss 0.690312385559082, classify 0.793040931224823\n",
      "Run 953600: train loss 0.6319069266319275, classify 0.8010217547416687\n",
      "Run 953600: val loss 0.6102260947227478, classify 0.8066202402114868\n",
      "Run 960000: train loss 0.5801870822906494, classify 0.826667845249176\n",
      "Run 960000: val loss 0.6142202615737915, classify 0.7995156049728394\n",
      "Run 966400: train loss 0.5433688759803772, classify 0.8258125185966492\n",
      "Run 966400: val loss 0.7008284330368042, classify 0.771228015422821\n",
      "Run 972800: train loss 0.5805816650390625, classify 0.8202903866767883\n",
      "Run 972800: val loss 0.6830419898033142, classify 0.7828385233879089\n",
      "Run 979200: train loss 0.600533127784729, classify 0.8324220180511475\n",
      "Run 979200: val loss 0.7380958199501038, classify 0.7784032225608826\n",
      "Run 985600: train loss 0.5695297718048096, classify 0.8140300512313843\n",
      "Run 985600: val loss 0.5877669453620911, classify 0.832454264163971\n",
      "Run 992000: train loss 0.5604218244552612, classify 0.829817533493042\n",
      "Run 992000: val loss 0.6754522323608398, classify 0.7987074851989746\n",
      "Run 998400: train loss 0.6168044805526733, classify 0.813112199306488\n",
      "Run 998400: val loss 0.6809641718864441, classify 0.7956286668777466\n",
      "Run 1004800: train loss 0.5652033090591431, classify 0.8280134201049805\n",
      "Run 1004800: val loss 0.6412195563316345, classify 0.8088889122009277\n",
      "Run 1011200: train loss 0.5218613743782043, classify 0.845734715461731\n",
      "Run 1011200: val loss 0.5997575521469116, classify 0.8189975023269653\n",
      "Run 1017600: train loss 0.6170475482940674, classify 0.8181120157241821\n",
      "Run 1017600: val loss 0.6507717370986938, classify 0.7975282669067383\n",
      "Run 1024000: train loss 0.5773282051086426, classify 0.821774959564209\n",
      "Run 1024000: val loss 0.6435887813568115, classify 0.7930395603179932\n",
      "Run 1030400: train loss 0.5366957187652588, classify 0.8337392210960388\n",
      "Run 1030400: val loss 0.7070881724357605, classify 0.7835319638252258\n",
      "Run 1036800: train loss 0.5657482743263245, classify 0.8219360709190369\n",
      "Run 1036800: val loss 0.7142636179924011, classify 0.7820020914077759\n",
      "Run 1043200: train loss 0.5540860295295715, classify 0.8296047449111938\n",
      "Run 1043200: val loss 0.5897557139396667, classify 0.8174989819526672\n",
      "Run 1049600: train loss 0.5674583315849304, classify 0.8206507563591003\n",
      "Run 1049600: val loss 0.6454759836196899, classify 0.8005865812301636\n",
      "Run 1056000: train loss 0.5161567330360413, classify 0.8413259387016296\n",
      "Run 1056000: val loss 0.7012367248535156, classify 0.7893819212913513\n",
      "Run 1062400: train loss 0.5607376098632812, classify 0.8240293860435486\n",
      "Run 1062400: val loss 0.6743258833885193, classify 0.7927064299583435\n",
      "Run 1068800: train loss 0.5741477012634277, classify 0.8368407487869263\n",
      "Run 1068800: val loss 0.6928310990333557, classify 0.7893943190574646\n",
      "Run 1075200: train loss 0.5160330533981323, classify 0.8326009511947632\n",
      "Run 1075200: val loss 0.6225592494010925, classify 0.7959294319152832\n",
      "Run 1081600: train loss 0.607174277305603, classify 0.8147730231285095\n",
      "Run 1081600: val loss 0.6284012794494629, classify 0.8079556226730347\n",
      "Run 1088000: train loss 0.5036571025848389, classify 0.8445233702659607\n",
      "Run 1088000: val loss 0.648801326751709, classify 0.7962063550949097\n",
      "Run 1094400: train loss 0.5395370125770569, classify 0.8447360992431641\n",
      "Run 1094400: val loss 0.6058908104896545, classify 0.7957293391227722\n",
      "Run 1100800: train loss 0.5989437103271484, classify 0.8198632001876831\n",
      "Run 1100800: val loss 0.6631155610084534, classify 0.7979018092155457\n",
      "Run 1107200: train loss 0.4934540092945099, classify 0.8436563611030579\n",
      "Run 1107200: val loss 0.5866690278053284, classify 0.8166269063949585\n",
      "Run 1113600: train loss 0.5302320122718811, classify 0.8370412588119507\n",
      "Run 1113600: val loss 0.6436588168144226, classify 0.8050076961517334\n",
      "Run 1120000: train loss 0.5500797629356384, classify 0.8384348750114441\n",
      "Run 1120000: val loss 0.6006969213485718, classify 0.8161343932151794\n",
      "Run 1126400: train loss 0.5213720798492432, classify 0.8378540277481079\n",
      "Run 1126400: val loss 0.6365638971328735, classify 0.7951995730400085\n",
      "Run 1132800: train loss 0.5266460180282593, classify 0.8252929449081421\n",
      "Run 1132800: val loss 0.6672924160957336, classify 0.7835999727249146\n",
      "Run 1139200: train loss 0.523263156414032, classify 0.8403537273406982\n",
      "Run 1139200: val loss 0.660737931728363, classify 0.7926021814346313\n",
      "Run 1145600: train loss 0.509338915348053, classify 0.8330633044242859\n",
      "Run 1145600: val loss 0.6619783639907837, classify 0.7933815121650696\n",
      "Run 1152000: train loss 0.5465463995933533, classify 0.8241739273071289\n",
      "Run 1152000: val loss 0.5954322218894958, classify 0.8099260926246643\n",
      "Run 1158400: train loss 0.5036415457725525, classify 0.8482558131217957\n",
      "Run 1158400: val loss 0.5715194344520569, classify 0.8217089772224426\n",
      "Run 1164800: train loss 0.5082470774650574, classify 0.8374470472335815\n",
      "Run 1164800: val loss 0.6776372790336609, classify 0.7913368344306946\n",
      "Run 1171200: train loss 0.5098326206207275, classify 0.845225989818573\n",
      "Run 1171200: val loss 0.6405516266822815, classify 0.7991207242012024\n",
      "Run 1177600: train loss 0.505371630191803, classify 0.8512722849845886\n",
      "Run 1177600: val loss 0.6639198660850525, classify 0.7951679825782776\n",
      "Run 1184000: train loss 0.4939478933811188, classify 0.8533891439437866\n",
      "Run 1184000: val loss 0.5876114368438721, classify 0.8247565031051636\n",
      "Run 1190400: train loss 0.49639251828193665, classify 0.8494477868080139\n",
      "Run 1190400: val loss 0.5759433507919312, classify 0.8222569823265076\n",
      "Run 1196800: train loss 0.49266117811203003, classify 0.8484920859336853\n",
      "Run 1196800: val loss 0.5953875184059143, classify 0.8103290796279907\n",
      "Run 1203200: train loss 0.5568925738334656, classify 0.8238793611526489\n",
      "Run 1203200: val loss 0.6442807912826538, classify 0.7908276319503784\n",
      "Run 1209600: train loss 0.4921722412109375, classify 0.8388752937316895\n",
      "Run 1209600: val loss 0.6166722774505615, classify 0.8074575662612915\n",
      "Run 1216000: train loss 0.44895198941230774, classify 0.867499828338623\n",
      "Run 1216000: val loss 0.5919419527053833, classify 0.8101521730422974\n",
      "Run 1222400: train loss 0.5552351474761963, classify 0.8349167108535767\n",
      "Run 1222400: val loss 0.6140336990356445, classify 0.8124192357063293\n",
      "Run 1228800: train loss 0.5419955849647522, classify 0.8380017876625061\n",
      "Run 1228800: val loss 0.5898004770278931, classify 0.8086801171302795\n",
      "Run 1235200: train loss 0.5133323073387146, classify 0.8421748876571655\n",
      "Run 1235200: val loss 0.6382389068603516, classify 0.8006379008293152\n",
      "Run 1241600: train loss 0.5036535263061523, classify 0.8408941626548767\n",
      "Run 1241600: val loss 0.622799813747406, classify 0.7988073825836182\n",
      "Run 1248000: train loss 0.5332956910133362, classify 0.820925235748291\n",
      "Run 1248000: val loss 0.5948121547698975, classify 0.8254334330558777\n",
      "Run 1254400: train loss 0.46936357021331787, classify 0.8556336164474487\n",
      "Run 1254400: val loss 0.628466010093689, classify 0.8017968535423279\n",
      "Run 1260800: train loss 0.4854765832424164, classify 0.849689781665802\n",
      "Run 1260800: val loss 0.710509717464447, classify 0.7905943393707275\n",
      "Run 1267200: train loss 0.5209484100341797, classify 0.845278799533844\n",
      "Run 1267200: val loss 0.6277854442596436, classify 0.8040317893028259\n",
      "Run 1273600: train loss 0.48012953996658325, classify 0.8565062284469604\n",
      "Run 1273600: val loss 0.5415612459182739, classify 0.827858567237854\n",
      "Run 1280000: train loss 0.49630799889564514, classify 0.8460988998413086\n",
      "Run 1280000: val loss 0.5719684362411499, classify 0.8205171823501587\n",
      "Run 1286400: train loss 0.5634060502052307, classify 0.8245170712471008\n",
      "Run 1286400: val loss 0.6087206602096558, classify 0.8100303411483765\n",
      "Run 1292800: train loss 0.5108759999275208, classify 0.844323456287384\n",
      "Run 1292800: val loss 0.6286002993583679, classify 0.8107925057411194\n",
      "Run 1299200: train loss 0.42017197608947754, classify 0.873173177242279\n",
      "Run 1299200: val loss 0.5813282132148743, classify 0.8269282579421997\n",
      "Run 1305600: train loss 0.47714975476264954, classify 0.848619282245636\n",
      "Run 1305600: val loss 0.5982354283332825, classify 0.8010688424110413\n",
      "Run 1312000: train loss 0.45447245240211487, classify 0.8586787581443787\n",
      "Run 1312000: val loss 0.6301134824752808, classify 0.8037173748016357\n",
      "Run 1318400: train loss 0.5198473334312439, classify 0.8527452349662781\n",
      "Run 1318400: val loss 0.5902990102767944, classify 0.8132762908935547\n",
      "Run 1324800: train loss 0.456609308719635, classify 0.862173855304718\n",
      "Run 1324800: val loss 0.5548603534698486, classify 0.8198572993278503\n",
      "Run 1331200: train loss 0.4588022828102112, classify 0.8615471124649048\n",
      "Run 1331200: val loss 0.6203979849815369, classify 0.8114598393440247\n",
      "Run 1337600: train loss 0.48036736249923706, classify 0.8551634550094604\n",
      "Run 1337600: val loss 0.6448026299476624, classify 0.791683554649353\n",
      "Run 1344000: train loss 0.46537715196609497, classify 0.8466892242431641\n",
      "Run 1344000: val loss 0.588851809501648, classify 0.8258642554283142\n",
      "Run 1350400: train loss 0.4552302956581116, classify 0.8605920672416687\n",
      "Run 1350400: val loss 0.5874780416488647, classify 0.8250066637992859\n",
      "Run 1356800: train loss 0.49698105454444885, classify 0.8375808000564575\n",
      "Run 1356800: val loss 0.5717066526412964, classify 0.8254655003547668\n",
      "Run 1363200: train loss 0.4498060941696167, classify 0.8601193428039551\n",
      "Run 1363200: val loss 0.5828322172164917, classify 0.8083503246307373\n",
      "Run 1369600: train loss 0.5150483846664429, classify 0.8406049013137817\n",
      "Run 1369600: val loss 0.544671893119812, classify 0.8314889669418335\n",
      "Run 1376000: train loss 0.5026121735572815, classify 0.8441515564918518\n",
      "Run 1376000: val loss 0.6178135871887207, classify 0.8055205941200256\n",
      "Run 1382400: train loss 0.451627641916275, classify 0.8606517910957336\n",
      "Run 1382400: val loss 0.5776623487472534, classify 0.8264822363853455\n",
      "Run 1388800: train loss 0.49158281087875366, classify 0.8485779762268066\n",
      "Run 1388800: val loss 0.5847934484481812, classify 0.8058830499649048\n",
      "Run 1395200: train loss 0.49516651034355164, classify 0.8507157564163208\n",
      "Run 1395200: val loss 0.5653819441795349, classify 0.8137989044189453\n",
      "Run 1401600: train loss 0.44789478182792664, classify 0.8784455060958862\n",
      "Run 1401600: val loss 0.5554817318916321, classify 0.8300305604934692\n",
      "Run 1408000: train loss 0.4652358293533325, classify 0.8535802364349365\n",
      "Run 1408000: val loss 0.5565495491027832, classify 0.8394395709037781\n",
      "Run 1414400: train loss 0.4756545424461365, classify 0.8411561250686646\n",
      "Run 1414400: val loss 0.6496424078941345, classify 0.7984187006950378\n",
      "Run 1420800: train loss 0.4927058219909668, classify 0.8515481948852539\n",
      "Run 1420800: val loss 0.5811036825180054, classify 0.8230879902839661\n",
      "Run 1427200: train loss 0.459148108959198, classify 0.8622444272041321\n",
      "Run 1427200: val loss 0.603215217590332, classify 0.8109949231147766\n",
      "Run 1433600: train loss 0.4755244553089142, classify 0.8576635122299194\n",
      "Run 1433600: val loss 0.6100221872329712, classify 0.8170326948165894\n",
      "Run 1440000: train loss 0.5079365372657776, classify 0.8491784334182739\n",
      "Run 1440000: val loss 0.6017407178878784, classify 0.8158186078071594\n",
      "Run 1446400: train loss 0.44831961393356323, classify 0.8582733869552612\n",
      "Run 1446400: val loss 0.6205188035964966, classify 0.8107843399047852\n",
      "Run 1452800: train loss 0.4382675886154175, classify 0.8675671219825745\n",
      "Run 1452800: val loss 0.5849626660346985, classify 0.8224934339523315\n",
      "Run 1459200: train loss 0.4409254491329193, classify 0.8697308301925659\n",
      "Run 1459200: val loss 0.5398982763290405, classify 0.8231104612350464\n",
      "Run 1465600: train loss 0.4537748098373413, classify 0.8641577959060669\n",
      "Run 1465600: val loss 0.5232171416282654, classify 0.8261919021606445\n",
      "Run 1472000: train loss 0.4777483344078064, classify 0.8627088665962219\n",
      "Run 1472000: val loss 0.570955753326416, classify 0.8267606496810913\n",
      "Run 1478400: train loss 0.4358295500278473, classify 0.8585876822471619\n",
      "Run 1478400: val loss 0.6069486737251282, classify 0.8162998557090759\n",
      "Run 1484800: train loss 0.5183876752853394, classify 0.8569333553314209\n",
      "Run 1484800: val loss 0.5961182117462158, classify 0.810907244682312\n",
      "Run 1491200: train loss 0.49094250798225403, classify 0.8522130846977234\n",
      "Run 1491200: val loss 0.575263261795044, classify 0.8148747682571411\n",
      "Run 1497600: train loss 0.4351467192173004, classify 0.87102872133255\n",
      "Run 1497600: val loss 0.5306783318519592, classify 0.8315481543540955\n",
      "Run 1504000: train loss 0.48059964179992676, classify 0.8434642553329468\n",
      "Run 1504000: val loss 0.563007116317749, classify 0.8275535702705383\n",
      "Run 1510400: train loss 0.4494245946407318, classify 0.8668025732040405\n",
      "Run 1510400: val loss 0.5788301825523376, classify 0.8218088746070862\n",
      "Run 1516800: train loss 0.44370341300964355, classify 0.8544779419898987\n",
      "Run 1516800: val loss 0.5517911911010742, classify 0.8410305976867676\n",
      "Run 1523200: train loss 0.43332791328430176, classify 0.8643726706504822\n",
      "Run 1523200: val loss 0.5500815510749817, classify 0.8359515070915222\n",
      "Run 1529600: train loss 0.44446006417274475, classify 0.858874499797821\n",
      "Run 1529600: val loss 0.54095458984375, classify 0.8344466090202332\n",
      "Run 1536000: train loss 0.429405152797699, classify 0.8709344863891602\n",
      "Run 1536000: val loss 0.5578720569610596, classify 0.831836998462677\n",
      "Run 1542400: train loss 0.41993868350982666, classify 0.8793070912361145\n",
      "Run 1542400: val loss 0.5918325185775757, classify 0.8170413970947266\n",
      "Run 1548800: train loss 0.43129763007164, classify 0.870816171169281\n",
      "Run 1548800: val loss 0.5500220656394958, classify 0.8382026553153992\n",
      "Run 1555200: train loss 0.477023184299469, classify 0.848625898361206\n",
      "Run 1555200: val loss 0.54781574010849, classify 0.8294159770011902\n",
      "Run 1561600: train loss 0.47031891345977783, classify 0.8540915250778198\n",
      "Run 1561600: val loss 0.5361599922180176, classify 0.8396416306495667\n",
      "Run 1568000: train loss 0.4432705342769623, classify 0.8753325939178467\n",
      "Run 1568000: val loss 0.5893117785453796, classify 0.8182607293128967\n",
      "Run 1574400: train loss 0.49659958481788635, classify 0.8457667231559753\n",
      "Run 1574400: val loss 0.6147720813751221, classify 0.8111254572868347\n",
      "Run 1580800: train loss 0.4036840796470642, classify 0.8828133940696716\n",
      "Run 1580800: val loss 0.5479381680488586, classify 0.8363893628120422\n",
      "Run 1587200: train loss 0.3795159161090851, classify 0.8795122504234314\n",
      "Run 1587200: val loss 0.527711808681488, classify 0.8380759954452515\n",
      "Run 1593600: train loss 0.4564559757709503, classify 0.8602704405784607\n",
      "Run 1593600: val loss 0.5997412800788879, classify 0.8204883933067322\n",
      "Run 1600000: train loss 0.4763493537902832, classify 0.8538494110107422\n",
      "Run 1600000: val loss 0.4979282021522522, classify 0.8419008851051331\n",
      "Run 1606400: train loss 0.4210837483406067, classify 0.8736570477485657\n",
      "Run 1606400: val loss 0.5151641368865967, classify 0.8347303867340088\n",
      "Run 1612800: train loss 0.44342631101608276, classify 0.8629854917526245\n",
      "Run 1612800: val loss 0.5803437232971191, classify 0.8210597634315491\n",
      "Run 1619200: train loss 0.4996054470539093, classify 0.8420835733413696\n",
      "Run 1619200: val loss 0.5497605800628662, classify 0.8190698623657227\n",
      "Run 1625600: train loss 0.4368239641189575, classify 0.867038369178772\n",
      "Run 1625600: val loss 0.5652580857276917, classify 0.8290243744850159\n",
      "Run 1632000: train loss 0.45610493421554565, classify 0.8560278415679932\n",
      "Run 1632000: val loss 0.530743420124054, classify 0.836630642414093\n",
      "Run 1638400: train loss 0.46279019117355347, classify 0.8492311239242554\n",
      "Run 1638400: val loss 0.5319275856018066, classify 0.8413358330726624\n",
      "Run 1644800: train loss 0.4359930157661438, classify 0.866027295589447\n",
      "Run 1644800: val loss 0.5616841316223145, classify 0.8237543702125549\n",
      "Run 1651200: train loss 0.4521840810775757, classify 0.8660240769386292\n",
      "Run 1651200: val loss 0.5443358421325684, classify 0.8180030584335327\n",
      "Run 1657600: train loss 0.4261375367641449, classify 0.8706467747688293\n",
      "Run 1657600: val loss 0.5178211331367493, classify 0.8337200284004211\n",
      "Run 1664000: train loss 0.41673946380615234, classify 0.8610155582427979\n",
      "Run 1664000: val loss 0.5580240488052368, classify 0.8233500719070435\n",
      "Run 1670400: train loss 0.4250737130641937, classify 0.8713967800140381\n",
      "Run 1670400: val loss 0.5423709750175476, classify 0.8340945839881897\n",
      "Run 1676800: train loss 0.4427656829357147, classify 0.8669918179512024\n",
      "Run 1676800: val loss 0.5269210338592529, classify 0.8401244878768921\n",
      "Run 1683200: train loss 0.3561154901981354, classify 0.892061710357666\n",
      "Run 1683200: val loss 0.5264595150947571, classify 0.8401285409927368\n",
      "Run 1689600: train loss 0.46704837679862976, classify 0.8617534637451172\n",
      "Run 1689600: val loss 0.5446604490280151, classify 0.8353213667869568\n",
      "Run 1696000: train loss 0.44365769624710083, classify 0.8597446084022522\n",
      "Run 1696000: val loss 0.5569731593132019, classify 0.8291850090026855\n",
      "Run 1702400: train loss 0.3953150808811188, classify 0.8844519257545471\n",
      "Run 1702400: val loss 0.5601396560668945, classify 0.8148970603942871\n",
      "Run 1708800: train loss 0.41546276211738586, classify 0.8688057661056519\n",
      "Run 1708800: val loss 0.5606070160865784, classify 0.8284884691238403\n",
      "Run 1715200: train loss 0.40762844681739807, classify 0.8738754391670227\n",
      "Run 1715200: val loss 0.4593234956264496, classify 0.8613240718841553\n",
      "Run 1721600: train loss 0.3934343159198761, classify 0.8719550967216492\n",
      "Run 1721600: val loss 0.5315595269203186, classify 0.8349102139472961\n",
      "Run 1728000: train loss 0.3887900710105896, classify 0.8894451260566711\n",
      "Run 1728000: val loss 0.571296215057373, classify 0.8119115829467773\n",
      "Run 1734400: train loss 0.4287123680114746, classify 0.867768406867981\n",
      "Run 1734400: val loss 0.47042346000671387, classify 0.8567880392074585\n",
      "Run 1740800: train loss 0.4152388572692871, classify 0.878804087638855\n",
      "Run 1740800: val loss 0.5193272829055786, classify 0.8348007202148438\n",
      "Run 1747200: train loss 0.43990203738212585, classify 0.86867755651474\n",
      "Run 1747200: val loss 0.5254108309745789, classify 0.8290696144104004\n",
      "Run 1753600: train loss 0.38754260540008545, classify 0.8823670148849487\n",
      "Run 1753600: val loss 0.5139769911766052, classify 0.8486458659172058\n",
      "Run 1760000: train loss 0.37827038764953613, classify 0.8731421232223511\n",
      "Run 1760000: val loss 0.5630085468292236, classify 0.811408281326294\n",
      "Run 1766400: train loss 0.41777920722961426, classify 0.8743224143981934\n",
      "Run 1766400: val loss 0.5276707410812378, classify 0.8361939787864685\n",
      "Run 1772800: train loss 0.39703455567359924, classify 0.8791327476501465\n",
      "Run 1772800: val loss 0.5474859476089478, classify 0.828960120677948\n",
      "Run 1779200: train loss 0.4250153601169586, classify 0.8737590312957764\n",
      "Run 1779200: val loss 0.5573899745941162, classify 0.8181607127189636\n",
      "Run 1785600: train loss 0.44352877140045166, classify 0.8615657091140747\n",
      "Run 1785600: val loss 0.5237621068954468, classify 0.8359149694442749\n",
      "Run 1792000: train loss 0.4460149109363556, classify 0.8610458374023438\n",
      "Run 1792000: val loss 0.5139605402946472, classify 0.8425475358963013\n",
      "Run 1798400: train loss 0.40186387300491333, classify 0.8735052347183228\n",
      "Run 1798400: val loss 0.4869900941848755, classify 0.8584011793136597\n",
      "Run 1804800: train loss 0.3691733479499817, classify 0.8946588635444641\n",
      "Run 1804800: val loss 0.5062374472618103, classify 0.8434484601020813\n",
      "Run 1811200: train loss 0.40132203698158264, classify 0.8852221369743347\n",
      "Run 1811200: val loss 0.5302085876464844, classify 0.8325718641281128\n",
      "Run 1817600: train loss 0.4123435914516449, classify 0.861164927482605\n",
      "Run 1817600: val loss 0.49912330508232117, classify 0.8322592377662659\n",
      "Run 1824000: train loss 0.40609991550445557, classify 0.8700127005577087\n",
      "Run 1824000: val loss 0.5244060158729553, classify 0.8361067771911621\n",
      "Run 1830400: train loss 0.47304248809814453, classify 0.8746570944786072\n",
      "Run 1830400: val loss 0.5301728844642639, classify 0.8363113403320312\n",
      "Run 1836800: train loss 0.3724287748336792, classify 0.891213595867157\n",
      "Run 1836800: val loss 0.5262669324874878, classify 0.8335264921188354\n",
      "Run 1843200: train loss 0.436332106590271, classify 0.8672443628311157\n",
      "Run 1843200: val loss 0.5431473255157471, classify 0.8358901143074036\n",
      "Run 1849600: train loss 0.3814663290977478, classify 0.8777090907096863\n",
      "Run 1849600: val loss 0.5643362998962402, classify 0.8293448090553284\n",
      "Run 1856000: train loss 0.4131634831428528, classify 0.8806841969490051\n",
      "Run 1856000: val loss 0.4980883002281189, classify 0.8398178219795227\n",
      "Run 1862400: train loss 0.3705056607723236, classify 0.8821624517440796\n",
      "Run 1862400: val loss 0.5347381234169006, classify 0.8348564505577087\n",
      "Run 1868800: train loss 0.4174923002719879, classify 0.8769040703773499\n",
      "Run 1868800: val loss 0.5058735609054565, classify 0.8373968005180359\n",
      "Run 1875200: train loss 0.3779597282409668, classify 0.8801681399345398\n",
      "Run 1875200: val loss 0.5121906995773315, classify 0.8346052765846252\n",
      "Run 1881600: train loss 0.4593629539012909, classify 0.8717151880264282\n",
      "Run 1881600: val loss 0.541449785232544, classify 0.8306307792663574\n",
      "Run 1888000: train loss 0.3846870958805084, classify 0.8886287808418274\n",
      "Run 1888000: val loss 0.4795423448085785, classify 0.8359608054161072\n",
      "Run 1894400: train loss 0.39219361543655396, classify 0.8779287934303284\n",
      "Run 1894400: val loss 0.48885905742645264, classify 0.8350540995597839\n",
      "Run 1900800: train loss 0.4246635138988495, classify 0.8719278573989868\n",
      "Run 1900800: val loss 0.5113888382911682, classify 0.8397071957588196\n",
      "Run 1907200: train loss 0.3851335942745209, classify 0.8868521451950073\n",
      "Run 1907200: val loss 0.5236480236053467, classify 0.8357964754104614\n",
      "Run 1913600: train loss 0.3588837683200836, classify 0.8946436643600464\n",
      "Run 1913600: val loss 0.4765538275241852, classify 0.8508252501487732\n",
      "Run 1920000: train loss 0.39054447412490845, classify 0.8814512491226196\n",
      "Run 1920000: val loss 0.547020435333252, classify 0.8288969993591309\n",
      "Run 1926400: train loss 0.3597548007965088, classify 0.8876878619194031\n",
      "Run 1926400: val loss 0.5279889702796936, classify 0.8372144103050232\n",
      "Run 1932800: train loss 0.3521839380264282, classify 0.8915072083473206\n",
      "Run 1932800: val loss 0.509712815284729, classify 0.8308348655700684\n",
      "Run 1939200: train loss 0.3683461844921112, classify 0.8839289546012878\n",
      "Run 1939200: val loss 0.5202075839042664, classify 0.8339809775352478\n",
      "Run 1945600: train loss 0.43369123339653015, classify 0.8670422434806824\n",
      "Run 1945600: val loss 0.5331595540046692, classify 0.8249269723892212\n",
      "Run 1952000: train loss 0.3835892975330353, classify 0.8976412415504456\n",
      "Run 1952000: val loss 0.5074861645698547, classify 0.8332439661026001\n",
      "Run 1958400: train loss 0.38632917404174805, classify 0.8833415508270264\n",
      "Run 1958400: val loss 0.5247408151626587, classify 0.8460850715637207\n",
      "Run 1964800: train loss 0.4179683327674866, classify 0.8728967308998108\n",
      "Run 1964800: val loss 0.48125654458999634, classify 0.8490373492240906\n",
      "Run 1971200: train loss 0.3689696192741394, classify 0.8775415420532227\n",
      "Run 1971200: val loss 0.5141723155975342, classify 0.829913854598999\n",
      "Run 1977600: train loss 0.3716122806072235, classify 0.8872652053833008\n",
      "Run 1977600: val loss 0.4871581792831421, classify 0.8522757291793823\n",
      "Run 1984000: train loss 0.39380189776420593, classify 0.8869022727012634\n",
      "Run 1984000: val loss 0.5366976857185364, classify 0.8307343125343323\n",
      "Run 1990400: train loss 0.35480087995529175, classify 0.8853257894515991\n",
      "Run 1990400: val loss 0.4660746157169342, classify 0.8499413728713989\n",
      "Run 1996800: train loss 0.35453134775161743, classify 0.8995981216430664\n",
      "Run 1996800: val loss 0.48755303025245667, classify 0.846606969833374\n",
      "Run 2003200: train loss 0.42772558331489563, classify 0.8601835370063782\n",
      "Run 2003200: val loss 0.5072401165962219, classify 0.8375596404075623\n",
      "Run 2009600: train loss 0.4094689190387726, classify 0.8746293783187866\n",
      "Run 2009600: val loss 0.528535008430481, classify 0.8309805989265442\n",
      "Run 2016000: train loss 0.37442708015441895, classify 0.8946955800056458\n",
      "Run 2016000: val loss 0.4872320294380188, classify 0.8498661518096924\n",
      "Run 2022400: train loss 0.3468035161495209, classify 0.8882335424423218\n",
      "Run 2022400: val loss 0.48803776502609253, classify 0.8481339812278748\n",
      "Run 2028800: train loss 0.3389328718185425, classify 0.9008018374443054\n",
      "Run 2028800: val loss 0.43625620007514954, classify 0.8623171448707581\n",
      "Run 2035200: train loss 0.3917911946773529, classify 0.8801976442337036\n",
      "Run 2035200: val loss 0.5174837708473206, classify 0.8358728885650635\n",
      "Run 2041600: train loss 0.4191943109035492, classify 0.868163526058197\n",
      "Run 2041600: val loss 0.5221747159957886, classify 0.8294391632080078\n",
      "Run 2048000: train loss 0.4287060499191284, classify 0.8706757426261902\n",
      "Run 2048000: val loss 0.5511234402656555, classify 0.8322197794914246\n",
      "Run 2054400: train loss 0.39946234226226807, classify 0.8728218674659729\n",
      "Run 2054400: val loss 0.4711996912956238, classify 0.8539958596229553\n",
      "Run 2060800: train loss 0.3731284737586975, classify 0.886995255947113\n",
      "Run 2060800: val loss 0.4916805922985077, classify 0.8462228178977966\n",
      "Run 2067200: train loss 0.36122867465019226, classify 0.8831769824028015\n",
      "Run 2067200: val loss 0.49695220589637756, classify 0.8441398739814758\n",
      "Run 2073600: train loss 0.3745124936103821, classify 0.8880854249000549\n",
      "Run 2073600: val loss 0.5024778246879578, classify 0.8396031856536865\n",
      "Run 2080000: train loss 0.3805774748325348, classify 0.8950545787811279\n",
      "Run 2080000: val loss 0.5013830065727234, classify 0.8366824984550476\n",
      "Run 2086400: train loss 0.3519144058227539, classify 0.8994892239570618\n",
      "Run 2086400: val loss 0.48664459586143494, classify 0.8532949686050415\n",
      "Run 2092800: train loss 0.4140968918800354, classify 0.8681269884109497\n",
      "Run 2092800: val loss 0.5083649158477783, classify 0.8387058973312378\n",
      "Run 2099200: train loss 0.38253140449523926, classify 0.8880777359008789\n",
      "Run 2099200: val loss 0.5014870762825012, classify 0.8548779487609863\n",
      "Run 2105600: train loss 0.38876286149024963, classify 0.872649610042572\n",
      "Run 2105600: val loss 0.5049893260002136, classify 0.8309871554374695\n",
      "Run 2112000: train loss 0.38488534092903137, classify 0.8896445631980896\n",
      "Run 2112000: val loss 0.4490602910518646, classify 0.8549059629440308\n",
      "Run 2118400: train loss 0.3476417660713196, classify 0.8944750428199768\n",
      "Run 2118400: val loss 0.496755450963974, classify 0.8474860191345215\n",
      "Run 2124800: train loss 0.3675496280193329, classify 0.8905328512191772\n",
      "Run 2124800: val loss 0.4675254225730896, classify 0.8517897725105286\n",
      "Run 2131200: train loss 0.32913169264793396, classify 0.89715176820755\n",
      "Run 2131200: val loss 0.474590003490448, classify 0.8513186573982239\n",
      "Run 2137600: train loss 0.3969438970088959, classify 0.8687633872032166\n",
      "Run 2137600: val loss 0.5126507878303528, classify 0.8401341438293457\n",
      "Run 2144000: train loss 0.37312403321266174, classify 0.8945883512496948\n",
      "Run 2144000: val loss 0.555429220199585, classify 0.8280460834503174\n",
      "Run 2150400: train loss 0.3276827931404114, classify 0.9045590758323669\n",
      "Run 2150400: val loss 0.4954793453216553, classify 0.8427268862724304\n",
      "Run 2156800: train loss 0.39093732833862305, classify 0.8896294236183167\n",
      "Run 2156800: val loss 0.5488406419754028, classify 0.8281019926071167\n",
      "Run 2163200: train loss 0.36008772253990173, classify 0.8803619742393494\n",
      "Run 2163200: val loss 0.45179837942123413, classify 0.8619388937950134\n",
      "Run 2169600: train loss 0.32262489199638367, classify 0.9013639688491821\n",
      "Run 2169600: val loss 0.48259279131889343, classify 0.8379778861999512\n",
      "Run 2176000: train loss 0.3606749176979065, classify 0.8830456137657166\n",
      "Run 2176000: val loss 0.46549224853515625, classify 0.8487344980239868\n",
      "Run 2182400: train loss 0.3758465349674225, classify 0.8864066004753113\n",
      "Run 2182400: val loss 0.4847915470600128, classify 0.8496178388595581\n",
      "Run 2188800: train loss 0.3624967634677887, classify 0.9002671837806702\n",
      "Run 2188800: val loss 0.47545871138572693, classify 0.8552815318107605\n",
      "Run 2195200: train loss 0.3627110421657562, classify 0.8872435688972473\n",
      "Run 2195200: val loss 0.49966928362846375, classify 0.8435508608818054\n",
      "Run 2201600: train loss 0.37865176796913147, classify 0.8836683034896851\n",
      "Run 2201600: val loss 0.48489195108413696, classify 0.8449150919914246\n",
      "Run 2208000: train loss 0.35987943410873413, classify 0.885217547416687\n",
      "Run 2208000: val loss 0.4746796190738678, classify 0.8448289632797241\n",
      "Run 2214400: train loss 0.33173081278800964, classify 0.9061747193336487\n",
      "Run 2214400: val loss 0.5375561714172363, classify 0.8233639001846313\n",
      "Run 2220800: train loss 0.3343699872493744, classify 0.888533353805542\n",
      "Run 2220800: val loss 0.4691566824913025, classify 0.8543903827667236\n",
      "Run 2227200: train loss 0.31753861904144287, classify 0.9064211845397949\n",
      "Run 2227200: val loss 0.47013863921165466, classify 0.8527060747146606\n",
      "Run 2233600: train loss 0.3291446268558502, classify 0.8975099325180054\n",
      "Run 2233600: val loss 0.4722839891910553, classify 0.8465543389320374\n",
      "Run 2240000: train loss 0.3474084138870239, classify 0.9015797972679138\n",
      "Run 2240000: val loss 0.4831724762916565, classify 0.8438166975975037\n",
      "Run 2246400: train loss 0.3912394940853119, classify 0.878177285194397\n",
      "Run 2246400: val loss 0.4515722990036011, classify 0.8577788472175598\n",
      "Run 2252800: train loss 0.33006277680397034, classify 0.8983573913574219\n",
      "Run 2252800: val loss 0.4701496958732605, classify 0.8545010089874268\n",
      "Run 2259200: train loss 0.3407161235809326, classify 0.8978915214538574\n",
      "Run 2259200: val loss 0.4783095419406891, classify 0.8514387011528015\n",
      "Run 2265600: train loss 0.3873395025730133, classify 0.8869970440864563\n",
      "Run 2265600: val loss 0.4994533956050873, classify 0.8449740409851074\n",
      "Run 2272000: train loss 0.35353806614875793, classify 0.8877784609794617\n",
      "Run 2272000: val loss 0.4722408950328827, classify 0.8431623578071594\n",
      "Run 2278400: train loss 0.31836503744125366, classify 0.9037920832633972\n",
      "Run 2278400: val loss 0.47752127051353455, classify 0.8467373251914978\n",
      "Run 2284800: train loss 0.3738498091697693, classify 0.8768720626831055\n",
      "Run 2284800: val loss 0.45356786251068115, classify 0.8561052083969116\n",
      "Run 2291200: train loss 0.3548901379108429, classify 0.8931249976158142\n",
      "Run 2291200: val loss 0.4872102737426758, classify 0.8366174101829529\n",
      "Run 2297600: train loss 0.3407212197780609, classify 0.8985310792922974\n",
      "Run 2297600: val loss 0.49317213892936707, classify 0.8434205651283264\n",
      "Run 2304000: train loss 0.301819384098053, classify 0.9108972549438477\n",
      "Run 2304000: val loss 0.45105335116386414, classify 0.8557725548744202\n",
      "Run 2310400: train loss 0.36912381649017334, classify 0.8847488760948181\n",
      "Run 2310400: val loss 0.416210412979126, classify 0.8661196827888489\n",
      "Run 2316800: train loss 0.3397235870361328, classify 0.9003888964653015\n",
      "Run 2316800: val loss 0.44494864344596863, classify 0.8388009667396545\n",
      "Run 2323200: train loss 0.3431902229785919, classify 0.8942736387252808\n",
      "Run 2323200: val loss 0.481938898563385, classify 0.8435799479484558\n",
      "Run 2329600: train loss 0.3144725561141968, classify 0.9034066200256348\n",
      "Run 2329600: val loss 0.4614517390727997, classify 0.8515394330024719\n",
      "Run 2336000: train loss 0.3688315451145172, classify 0.8871185779571533\n",
      "Run 2336000: val loss 0.4581695795059204, classify 0.8547990322113037\n",
      "Run 2342400: train loss 0.35493454337120056, classify 0.8930618166923523\n",
      "Run 2342400: val loss 0.47673454880714417, classify 0.8536006808280945\n",
      "Run 2348800: train loss 0.3545028865337372, classify 0.8988860249519348\n",
      "Run 2348800: val loss 0.4809557795524597, classify 0.846405029296875\n",
      "Run 2355200: train loss 0.36820629239082336, classify 0.8849612474441528\n",
      "Run 2355200: val loss 0.4523469805717468, classify 0.8561112880706787\n",
      "Run 2361600: train loss 0.35981863737106323, classify 0.8946343064308167\n",
      "Run 2361600: val loss 0.4892558455467224, classify 0.8436461687088013\n",
      "Run 2368000: train loss 0.3438193202018738, classify 0.8951138854026794\n",
      "Run 2368000: val loss 0.48458001017570496, classify 0.8478841781616211\n",
      "Run 2374400: train loss 0.3309643268585205, classify 0.8964729905128479\n",
      "Run 2374400: val loss 0.5329088568687439, classify 0.8278368711471558\n",
      "Run 2380800: train loss 0.3761219084262848, classify 0.887212336063385\n",
      "Run 2380800: val loss 0.4861239492893219, classify 0.8431790471076965\n",
      "Run 2387200: train loss 0.3267398476600647, classify 0.8988275527954102\n",
      "Run 2387200: val loss 0.5120401978492737, classify 0.8368231654167175\n",
      "Run 2393600: train loss 0.34014859795570374, classify 0.904975950717926\n",
      "Run 2393600: val loss 0.4419006109237671, classify 0.8576763868331909\n",
      "Run 2400000: train loss 0.36094510555267334, classify 0.8864997029304504\n",
      "Run 2400000: val loss 0.4841618835926056, classify 0.8512013554573059\n",
      "Run 2406400: train loss 0.3416074216365814, classify 0.8955721855163574\n",
      "Run 2406400: val loss 0.4496364891529083, classify 0.8688966035842896\n",
      "Run 2412800: train loss 0.3277207314968109, classify 0.8967058062553406\n",
      "Run 2412800: val loss 0.45782285928726196, classify 0.8622293472290039\n",
      "Run 2419200: train loss 0.2932342290878296, classify 0.9130063652992249\n",
      "Run 2419200: val loss 0.4889147877693176, classify 0.8474752306938171\n",
      "Run 2425600: train loss 0.31963059306144714, classify 0.8998079895973206\n",
      "Run 2425600: val loss 0.4217560589313507, classify 0.8617225289344788\n",
      "Run 2432000: train loss 0.33707675337791443, classify 0.9063758254051208\n",
      "Run 2432000: val loss 0.4961250424385071, classify 0.84336256980896\n",
      "Run 2438400: train loss 0.30781465768814087, classify 0.9051117300987244\n",
      "Run 2438400: val loss 0.48025351762771606, classify 0.8482808470726013\n",
      "Run 2444800: train loss 0.3667415678501129, classify 0.897230327129364\n",
      "Run 2444800: val loss 0.461905300617218, classify 0.8475112915039062\n",
      "Run 2451200: train loss 0.3247746229171753, classify 0.9095790982246399\n",
      "Run 2451200: val loss 0.4255368113517761, classify 0.8609753847122192\n",
      "Run 2457600: train loss 0.3326234519481659, classify 0.8940889835357666\n",
      "Run 2457600: val loss 0.4594464600086212, classify 0.8659341931343079\n",
      "Run 2464000: train loss 0.32760894298553467, classify 0.8913264870643616\n",
      "Run 2464000: val loss 0.4535228908061981, classify 0.8514163494110107\n",
      "Run 2470400: train loss 0.3770105242729187, classify 0.886599063873291\n",
      "Run 2470400: val loss 0.446516215801239, classify 0.8641557693481445\n",
      "Run 2476800: train loss 0.3271152973175049, classify 0.89736008644104\n",
      "Run 2476800: val loss 0.4447929263114929, classify 0.854924201965332\n",
      "Run 2483200: train loss 0.32212257385253906, classify 0.9025111794471741\n",
      "Run 2483200: val loss 0.4694575369358063, classify 0.8577744960784912\n",
      "Run 2489600: train loss 0.31742003560066223, classify 0.906778872013092\n",
      "Run 2489600: val loss 0.4748867154121399, classify 0.8471835255622864\n",
      "Run 2496000: train loss 0.34444695711135864, classify 0.8906630277633667\n",
      "Run 2496000: val loss 0.46897023916244507, classify 0.8492791056632996\n",
      "Run 2502400: train loss 0.31233879923820496, classify 0.9106729626655579\n",
      "Run 2502400: val loss 0.4472079277038574, classify 0.8591881990432739\n",
      "Run 2508800: train loss 0.3180633783340454, classify 0.9032776951789856\n",
      "Run 2508800: val loss 0.4342346489429474, classify 0.8455459475517273\n",
      "Run 2515200: train loss 0.3164661228656769, classify 0.9036867618560791\n",
      "Run 2515200: val loss 0.4716658294200897, classify 0.8535622954368591\n",
      "Run 2521600: train loss 0.30932632088661194, classify 0.9100717902183533\n",
      "Run 2521600: val loss 0.46972739696502686, classify 0.8470267653465271\n",
      "Run 2528000: train loss 0.3302757740020752, classify 0.9032347798347473\n",
      "Run 2528000: val loss 0.46735069155693054, classify 0.8437292575836182\n",
      "Run 2534400: train loss 0.35729873180389404, classify 0.8932536244392395\n",
      "Run 2534400: val loss 0.4443615972995758, classify 0.8564791679382324\n",
      "Run 2540800: train loss 0.34382304549217224, classify 0.8941442966461182\n",
      "Run 2540800: val loss 0.4629823565483093, classify 0.8507638573646545\n",
      "Run 2547200: train loss 0.3327333331108093, classify 0.8958452939987183\n",
      "Run 2547200: val loss 0.4385128319263458, classify 0.8636051416397095\n",
      "Run 2553600: train loss 0.367377907037735, classify 0.8973380327224731\n",
      "Run 2553600: val loss 0.5034388303756714, classify 0.8403294682502747\n",
      "Run 2560000: train loss 0.3194316327571869, classify 0.9031264781951904\n",
      "Run 2560000: val loss 0.4244319796562195, classify 0.8617433309555054\n",
      "Run 2566400: train loss 0.2930064797401428, classify 0.9087011814117432\n",
      "Run 2566400: val loss 0.42440128326416016, classify 0.865928053855896\n",
      "Run 2572800: train loss 0.324529230594635, classify 0.896364152431488\n",
      "Run 2572800: val loss 0.4415350556373596, classify 0.8576069474220276\n",
      "Run 2579200: train loss 0.28961580991744995, classify 0.9159805774688721\n",
      "Run 2579200: val loss 0.4063563048839569, classify 0.8656879663467407\n",
      "Run 2585600: train loss 0.3079439401626587, classify 0.9161326885223389\n",
      "Run 2585600: val loss 0.4236939251422882, classify 0.8734189867973328\n",
      "Run 2592000: train loss 0.3282638192176819, classify 0.8962447643280029\n",
      "Run 2592000: val loss 0.4706175625324249, classify 0.846780002117157\n",
      "Run 2598400: train loss 0.2902502119541168, classify 0.9164438247680664\n",
      "Run 2598400: val loss 0.4884858727455139, classify 0.8468309640884399\n",
      "Run 2604800: train loss 0.3384372591972351, classify 0.9003526568412781\n",
      "Run 2604800: val loss 0.40780746936798096, classify 0.8596130609512329\n",
      "Run 2611200: train loss 0.32735735177993774, classify 0.9017317295074463\n",
      "Run 2611200: val loss 0.4391586482524872, classify 0.8511558771133423\n",
      "Run 2617600: train loss 0.2978673577308655, classify 0.9041022658348083\n",
      "Run 2617600: val loss 0.4717869162559509, classify 0.8494324088096619\n",
      "Run 2624000: train loss 0.3117907643318176, classify 0.904975950717926\n",
      "Run 2624000: val loss 0.43482592701911926, classify 0.8663459420204163\n",
      "Run 2630400: train loss 0.310636967420578, classify 0.9103053212165833\n",
      "Run 2630400: val loss 0.4303789734840393, classify 0.8652821183204651\n",
      "Run 2636800: train loss 0.30854684114456177, classify 0.9021340608596802\n",
      "Run 2636800: val loss 0.40939706563949585, classify 0.8663155436515808\n",
      "Run 2643200: train loss 0.28726091980934143, classify 0.9160761833190918\n",
      "Run 2643200: val loss 0.46232929825782776, classify 0.8525758385658264\n",
      "Run 2649600: train loss 0.33525794744491577, classify 0.8927860260009766\n",
      "Run 2649600: val loss 0.48053720593452454, classify 0.8414382934570312\n",
      "Run 2656000: train loss 0.37018632888793945, classify 0.9005400538444519\n",
      "Run 2656000: val loss 0.45137688517570496, classify 0.8549192547798157\n",
      "Run 2662400: train loss 0.271712064743042, classify 0.912675678730011\n",
      "Run 2662400: val loss 0.4570552110671997, classify 0.8452036380767822\n",
      "Run 2668800: train loss 0.3374101221561432, classify 0.8950521349906921\n",
      "Run 2668800: val loss 0.45703908801078796, classify 0.8522279858589172\n",
      "Run 2675200: train loss 0.3327743411064148, classify 0.8994786739349365\n",
      "Run 2675200: val loss 0.43106886744499207, classify 0.8585675954818726\n",
      "Run 2681600: train loss 0.29163792729377747, classify 0.9043055176734924\n",
      "Run 2681600: val loss 0.4466584026813507, classify 0.8424298167228699\n",
      "Run 2688000: train loss 0.3238178789615631, classify 0.9115701913833618\n",
      "Run 2688000: val loss 0.4625824987888336, classify 0.8497850894927979\n",
      "Run 2694400: train loss 0.31338372826576233, classify 0.9054650068283081\n",
      "Run 2694400: val loss 0.44863107800483704, classify 0.8565639853477478\n",
      "Run 2700800: train loss 0.3164702355861664, classify 0.9021093249320984\n",
      "Run 2700800: val loss 0.3975241184234619, classify 0.8742519617080688\n",
      "Run 2707200: train loss 0.2775411605834961, classify 0.9109672904014587\n",
      "Run 2707200: val loss 0.423170268535614, classify 0.8627874851226807\n",
      "Run 2713600: train loss 0.32372963428497314, classify 0.8933404684066772\n",
      "Run 2713600: val loss 0.401770681142807, classify 0.8638450503349304\n",
      "Run 2720000: train loss 0.2912604510784149, classify 0.9233387112617493\n",
      "Run 2720000: val loss 0.4626157581806183, classify 0.8529468774795532\n",
      "Run 2726400: train loss 0.30196070671081543, classify 0.9051316380500793\n",
      "Run 2726400: val loss 0.4513949155807495, classify 0.8501957058906555\n",
      "Run 2732800: train loss 0.29022786021232605, classify 0.9131540656089783\n",
      "Run 2732800: val loss 0.4505760967731476, classify 0.8621613383293152\n",
      "Run 2739200: train loss 0.3166049122810364, classify 0.9103412628173828\n",
      "Run 2739200: val loss 0.4693620502948761, classify 0.8481879830360413\n",
      "Run 2745600: train loss 0.30565962195396423, classify 0.897453248500824\n",
      "Run 2745600: val loss 0.4805660545825958, classify 0.8461201190948486\n",
      "Run 2752000: train loss 0.31636515259742737, classify 0.9030810594558716\n",
      "Run 2752000: val loss 0.4491846263408661, classify 0.8586841821670532\n",
      "Run 2758400: train loss 0.2888554632663727, classify 0.9157270789146423\n",
      "Run 2758400: val loss 0.4215306043624878, classify 0.8585872650146484\n",
      "Run 2764800: train loss 0.3147513270378113, classify 0.9140334129333496\n",
      "Run 2764800: val loss 0.40977177023887634, classify 0.8623186349868774\n",
      "Run 2771200: train loss 0.3030364215373993, classify 0.9100467562675476\n",
      "Run 2771200: val loss 0.4193079471588135, classify 0.864123523235321\n",
      "Run 2777600: train loss 0.3378000259399414, classify 0.8900312185287476\n",
      "Run 2777600: val loss 0.3913997411727905, classify 0.8688973784446716\n",
      "Run 2784000: train loss 0.3022737205028534, classify 0.9142102599143982\n",
      "Run 2784000: val loss 0.4303332269191742, classify 0.8582801818847656\n",
      "Run 2790400: train loss 0.31969472765922546, classify 0.9026328921318054\n",
      "Run 2790400: val loss 0.42924052476882935, classify 0.8544614911079407\n",
      "Run 2796800: train loss 0.3234788179397583, classify 0.9027993083000183\n",
      "Run 2796800: val loss 0.4762350916862488, classify 0.8528218865394592\n",
      "Run 2803200: train loss 0.33423879742622375, classify 0.9048032164573669\n",
      "Run 2803200: val loss 0.4381629526615143, classify 0.8582620620727539\n",
      "Run 2809600: train loss 0.27073267102241516, classify 0.923523485660553\n",
      "Run 2809600: val loss 0.4304363429546356, classify 0.8586513996124268\n",
      "Run 2816000: train loss 0.282770574092865, classify 0.9129329323768616\n",
      "Run 2816000: val loss 0.4179210662841797, classify 0.8678957223892212\n",
      "Run 2822400: train loss 0.28143420815467834, classify 0.9134775400161743\n",
      "Run 2822400: val loss 0.4609820544719696, classify 0.8476982712745667\n",
      "Run 2828800: train loss 0.284130334854126, classify 0.9165510535240173\n",
      "Run 2828800: val loss 0.4398733377456665, classify 0.8674889802932739\n",
      "Run 2835200: train loss 0.3116437792778015, classify 0.9052346348762512\n",
      "Run 2835200: val loss 0.403951495885849, classify 0.8735828995704651\n",
      "Run 2841600: train loss 0.3088257312774658, classify 0.9068662524223328\n",
      "Run 2841600: val loss 0.44332563877105713, classify 0.8538497090339661\n",
      "Run 2848000: train loss 0.31720107793807983, classify 0.8958746194839478\n",
      "Run 2848000: val loss 0.4454474449157715, classify 0.8656112551689148\n",
      "Run 2854400: train loss 0.29054978489875793, classify 0.914935290813446\n",
      "Run 2854400: val loss 0.4634753465652466, classify 0.8510122895240784\n",
      "Run 2860800: train loss 0.27009955048561096, classify 0.9120131134986877\n",
      "Run 2860800: val loss 0.43323612213134766, classify 0.8586129546165466\n",
      "Run 2867200: train loss 0.2901124954223633, classify 0.9200188517570496\n",
      "Run 2867200: val loss 0.43935006856918335, classify 0.8536809086799622\n",
      "Run 2873600: train loss 0.2889649271965027, classify 0.9150719046592712\n",
      "Run 2873600: val loss 0.41042637825012207, classify 0.8734554052352905\n",
      "Run 2880000: train loss 0.3013376295566559, classify 0.9092910289764404\n",
      "Run 2880000: val loss 0.42318829894065857, classify 0.8628630042076111\n",
      "Run 2886400: train loss 0.26665011048316956, classify 0.921267569065094\n",
      "Run 2886400: val loss 0.4837634563446045, classify 0.8403785228729248\n",
      "Run 2892800: train loss 0.3405553996562958, classify 0.8918132185935974\n",
      "Run 2892800: val loss 0.41139814257621765, classify 0.8723487854003906\n",
      "Run 2899200: train loss 0.30262491106987, classify 0.9073715209960938\n",
      "Run 2899200: val loss 0.44980210065841675, classify 0.8494415879249573\n",
      "Run 2905600: train loss 0.3037961721420288, classify 0.9122493863105774\n",
      "Run 2905600: val loss 0.4577052891254425, classify 0.8590361475944519\n",
      "Run 2912000: train loss 0.28206130862236023, classify 0.9201837182044983\n",
      "Run 2912000: val loss 0.3946317136287689, classify 0.8604505658149719\n",
      "Run 2918400: train loss 0.270395427942276, classify 0.923134446144104\n",
      "Run 2918400: val loss 0.3698787987232208, classify 0.8674026727676392\n",
      "Run 2924800: train loss 0.27275174856185913, classify 0.9232664704322815\n",
      "Run 2924800: val loss 0.43999895453453064, classify 0.8620288968086243\n",
      "Run 2931200: train loss 0.2996979355812073, classify 0.8979235887527466\n",
      "Run 2931200: val loss 0.4092883765697479, classify 0.8658859133720398\n",
      "Run 2937600: train loss 0.3291327953338623, classify 0.8984103202819824\n",
      "Run 2937600: val loss 0.42735207080841064, classify 0.8637474775314331\n",
      "Run 2944000: train loss 0.2814924120903015, classify 0.9189499616622925\n",
      "Run 2944000: val loss 0.4615953862667084, classify 0.8449391722679138\n",
      "Run 2950400: train loss 0.27894556522369385, classify 0.9174957871437073\n",
      "Run 2950400: val loss 0.44400525093078613, classify 0.848294734954834\n",
      "Run 2956800: train loss 0.28303995728492737, classify 0.9263542294502258\n",
      "Run 2956800: val loss 0.4695860743522644, classify 0.8596882224082947\n",
      "Run 2963200: train loss 0.2921241223812103, classify 0.9110393524169922\n",
      "Run 2963200: val loss 0.4197441339492798, classify 0.8599749803543091\n",
      "Run 2969600: train loss 0.2965981960296631, classify 0.9206869602203369\n",
      "Run 2969600: val loss 0.4413786232471466, classify 0.8647158145904541\n",
      "Run 2976000: train loss 0.296269029378891, classify 0.915234386920929\n",
      "Run 2976000: val loss 0.4171540141105652, classify 0.8712846636772156\n",
      "Run 2982400: train loss 0.3020355999469757, classify 0.9137257933616638\n",
      "Run 2982400: val loss 0.4406284689903259, classify 0.8544585704803467\n",
      "Run 2988800: train loss 0.30060338973999023, classify 0.9193076491355896\n",
      "Run 2988800: val loss 0.4299030005931854, classify 0.8703425526618958\n",
      "Run 2995200: train loss 0.2921752333641052, classify 0.9184037446975708\n",
      "Run 2995200: val loss 0.4062007963657379, classify 0.8713181614875793\n",
      "Run 3001600: train loss 0.26672041416168213, classify 0.9193947315216064\n",
      "Run 3001600: val loss 0.3956393301486969, classify 0.8684995174407959\n",
      "Run 3008000: train loss 0.29318270087242126, classify 0.907148540019989\n",
      "Run 3008000: val loss 0.3985885977745056, classify 0.869147002696991\n",
      "Run 3014400: train loss 0.2805311381816864, classify 0.9156662821769714\n",
      "Run 3014400: val loss 0.46655067801475525, classify 0.8500453233718872\n",
      "Run 3020800: train loss 0.28604766726493835, classify 0.9197983145713806\n",
      "Run 3020800: val loss 0.41901907324790955, classify 0.8609850406646729\n",
      "Run 3027200: train loss 0.2697523534297943, classify 0.9167788028717041\n",
      "Run 3027200: val loss 0.39572882652282715, classify 0.8674933910369873\n",
      "Run 3033600: train loss 0.28435033559799194, classify 0.9074024558067322\n",
      "Run 3033600: val loss 0.44821038842201233, classify 0.8564828634262085\n",
      "Run 3040000: train loss 0.2919251322746277, classify 0.9129602313041687\n",
      "Run 3040000: val loss 0.41485852003097534, classify 0.8632097244262695\n",
      "Run 3046400: train loss 0.3095686733722687, classify 0.893801212310791\n",
      "Run 3046400: val loss 0.46017521619796753, classify 0.8543024659156799\n",
      "Run 3052800: train loss 0.28667008876800537, classify 0.9139453172683716\n",
      "Run 3052800: val loss 0.42869922518730164, classify 0.8596448302268982\n",
      "Run 3059200: train loss 0.28439486026763916, classify 0.9196174144744873\n",
      "Run 3059200: val loss 0.3871505856513977, classify 0.8696478009223938\n",
      "Run 3065600: train loss 0.28181347250938416, classify 0.9157791137695312\n",
      "Run 3065600: val loss 0.45478948950767517, classify 0.8563474416732788\n",
      "Run 3072000: train loss 0.3103254437446594, classify 0.904115617275238\n",
      "Run 3072000: val loss 0.42471519112586975, classify 0.8569706678390503\n",
      "Run 3078400: train loss 0.2800699472427368, classify 0.9173438549041748\n",
      "Run 3078400: val loss 0.45714816451072693, classify 0.8415653705596924\n",
      "Run 3084800: train loss 0.236232727766037, classify 0.9262857437133789\n",
      "Run 3084800: val loss 0.3981148302555084, classify 0.8700196743011475\n",
      "Run 3091200: train loss 0.30455392599105835, classify 0.9098060727119446\n",
      "Run 3091200: val loss 0.4034505784511566, classify 0.8636473417282104\n",
      "Run 3097600: train loss 0.31706154346466064, classify 0.9052636027336121\n",
      "Run 3097600: val loss 0.4047507047653198, classify 0.8665616512298584\n",
      "Run 3104000: train loss 0.2806313931941986, classify 0.9146527051925659\n",
      "Run 3104000: val loss 0.3561185896396637, classify 0.8779264092445374\n",
      "Run 3110400: train loss 0.2676810026168823, classify 0.9228023290634155\n",
      "Run 3110400: val loss 0.415223628282547, classify 0.8738376498222351\n",
      "Run 3116800: train loss 0.316265732049942, classify 0.8980376720428467\n",
      "Run 3116800: val loss 0.4455520510673523, classify 0.8463889360427856\n",
      "Run 3123200: train loss 0.25775647163391113, classify 0.9129769802093506\n",
      "Run 3123200: val loss 0.3770241439342499, classify 0.869168758392334\n",
      "Run 3129600: train loss 0.27104219794273376, classify 0.9253827929496765\n",
      "Run 3129600: val loss 0.42147713899612427, classify 0.8646246790885925\n",
      "Run 3136000: train loss 0.2621423006057739, classify 0.9268925786018372\n",
      "Run 3136000: val loss 0.3892357349395752, classify 0.8761504888534546\n",
      "Run 3142400: train loss 0.24922896921634674, classify 0.9267534017562866\n",
      "Run 3142400: val loss 0.39086395502090454, classify 0.8724879622459412\n",
      "Run 3148800: train loss 0.26625725626945496, classify 0.9173895716667175\n",
      "Run 3148800: val loss 0.3909929096698761, classify 0.8706778287887573\n",
      "Run 3155200: train loss 0.2939026951789856, classify 0.9083808660507202\n",
      "Run 3155200: val loss 0.40069156885147095, classify 0.8735660314559937\n",
      "Run 3161600: train loss 0.2634805738925934, classify 0.9188253283500671\n",
      "Run 3161600: val loss 0.3978290259838104, classify 0.8658246994018555\n",
      "Run 3168000: train loss 0.2709212005138397, classify 0.9059547781944275\n",
      "Run 3168000: val loss 0.4198574721813202, classify 0.8618773818016052\n",
      "Run 3174400: train loss 0.29476815462112427, classify 0.915215253829956\n",
      "Run 3174400: val loss 0.46217185258865356, classify 0.8442390561103821\n",
      "Run 3180800: train loss 0.27183860540390015, classify 0.9224907755851746\n",
      "Run 3180800: val loss 0.3968268036842346, classify 0.8743993639945984\n",
      "Run 3187200: train loss 0.3037504255771637, classify 0.9005443453788757\n",
      "Run 3187200: val loss 0.42458561062812805, classify 0.8573513031005859\n",
      "Run 3193600: train loss 0.30863022804260254, classify 0.9018875360488892\n",
      "Run 3193600: val loss 0.3847944438457489, classify 0.8706019520759583\n",
      "Run 3200000: train loss 0.2600517272949219, classify 0.9263579845428467\n",
      "Run 3200000: val loss 0.4148319363594055, classify 0.8587253093719482\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "# with jax.disable_jit():\n",
    "for i in range(1, total + 1):\n",
    "    u, v = (i - 1) * batch_size, i * batch_size\n",
    "    # u, v = 0, batch_size # Overfit for debugging\n",
    "    xs_t, data_indices_t = prepare_batch(xs_train, data_indices_train, u, v)\n",
    "    xs_v, data_indices_v = prepare_batch(xs_val, data_indices_val, u, v)\n",
    "    if i < 2001:\n",
    "        # Warmup phase (i from 1 to 2001)\n",
    "        model.adam_lr = 2e-4\n",
    "    else:\n",
    "        # Exponential learning rate decay from 8e-5 down to 2e-5 (i from 2001 to 6001)\n",
    "        model.adam_lr = 8e-5 * jnp.power(2.0, -min(i - 2001, 4000) / 2000)\n",
    "    model.adamw_update(xs_t, data_indices_t)\n",
    "    # Print current train and validation loss\n",
    "    model.update_loss_and_classify(model.train_arr, xs_t, data_indices_t)\n",
    "    model.update_loss_and_classify(model.val_arr, xs_v, data_indices_v)\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Run {i * batch_size}: train loss {model.train_arr[-1][0]}, classify {model.train_arr[-1][1]}\")\n",
    "        print(f\"Run {i * batch_size}: val loss {model.val_arr[-1][0]}, classify {model.val_arr[-1][1]}\")\n",
    "\n",
    "with open(f\"checkpoints/modelcheck-{total}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "fig, (ax_l, ax_r) = plt.subplots(1, 2)\n",
    "ax_l.plot(batch_size * np.arange(0, total + 1), [ t[0] for t in model.train_arr ], label = \"train\")\n",
    "ax_l.plot(batch_size * np.arange(0, total + 1), [ t[0] for t in model.val_arr ], label = \"val\")\n",
    "ax_r.plot(batch_size * np.arange(0, total + 1), [ t[1] for t in model.train_arr ], label = \"train\")\n",
    "ax_r.plot(batch_size * np.arange(0, total + 1), [ t[1] for t in model.val_arr ], label = \"val\")\n",
    "ax_l.legend()\n",
    "ax_r.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final: test loss 0.42871150374412537, classify 0.8670247793197632\n"
     ]
    }
   ],
   "source": [
    "# Report test error\n",
    "\n",
    "# Load checkpoint (not necessary if training loop was executed)\n",
    "with open(f\"checkpoints/modelcheck-{total}.pkl\", \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "loop_num = xs_test.shape[0] // batch_size\n",
    "\n",
    "model.test_loss = 0.0\n",
    "model.test_classify = 0.0\n",
    "\n",
    "for i in range(0, loop_num):\n",
    "    u, v = i * batch_size, (i + 1) * batch_size\n",
    "    xs, data_indices = prepare_batch(xs_test, data_indices_test, u, v)\n",
    "    model.test_loss += model.forward_loss_batched(xs, data_indices)\n",
    "    model.test_classify += model.forward_classify_batched(xs, data_indices)\n",
    "\n",
    "model.test_loss /= loop_num\n",
    "model.test_classify /= loop_num\n",
    "\n",
    "with open(f\"checkpoints/modelcheck-{total}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "print(f\"Final: test loss {model.test_loss}, classify {model.test_classify}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
